{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88501531",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.4.4-cp38-cp38-win_amd64.whl (270 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\ad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.60.0)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.6.2 regex-2021.4.4\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80927278",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a295783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 10\n",
      "[\"You ain't sure yet.But I'm for ya.\", 'All I could want, all I could wish for.', 'Nights alone that we miss more.', 'And days we save as souvenirs.', \"There's no time, I wanna make more time.\", 'And give you my whole life.', 'I left my girl, I’m in Mallorca.', 'Hate to leave her, call it torture.', 'Remember when I couldn’t hold her.', 'Left her baggage for Rimowa.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화 : sent_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample =\"You ain't sure yet.\\\n",
    "But I'm for ya. \\\n",
    "All I could want, all I could wish for. \\\n",
    "Nights alone that we miss more. \\\n",
    "And days we save as souvenirs. \\\n",
    "There's no time, I wanna make more time. \\\n",
    "And give you my whole life. \\\n",
    "I left my girl, I’m in Mallorca. \\\n",
    "Hate to leave her, call it torture. \\\n",
    "Remember when I couldn’t hold her. \\\n",
    "Left her baggage for Rimowa.\"\n",
    "### \" \" 만 쓰면 \\로 문장이 이어진다는 표시 해줘야함\n",
    "### \"\"\" \"\"\" 를 쓸 경우엔 \\를 하지 않아도 문장이 이어짐\n",
    "### (.)콜론을 기준으로 문장을 나눔\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "## 문장을 기준으로 나눠라\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8c274a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 5\n",
      "['You', 'ai', \"n't\", 'sure', 'yet']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화 : word_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"You ain't sure yet\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "## 단어를 기준으로 나눠라\n",
    "\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4629b",
   "metadata": {},
   "source": [
    "### 실습 : 텍스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf09fb8",
   "metadata": {},
   "source": [
    "#### 문장 별 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3bdd9e06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 10\n",
      "[['You', 'ai', \"n't\", 'sure', 'yet.But', 'I', \"'m\", 'for', 'ya', '.'], ['All', 'I', 'could', 'want', ',', 'all', 'I', 'could', 'wish', 'for', '.'], ['Nights', 'alone', 'that', 'we', 'miss', 'more', '.'], ['And', 'days', 'we', 'save', 'as', 'souvenirs', '.'], ['There', \"'s\", 'no', 'time', ',', 'I', 'wan', 'na', 'make', 'more', 'time', '.'], ['And', 'give', 'you', 'my', 'whole', 'life', '.'], ['I', 'left', 'my', 'girl', ',', 'I', '’', 'm', 'in', 'Mallorca', '.'], ['Hate', 'to', 'leave', 'her', ',', 'call', 'it', 'torture', '.'], ['Remember', 'when', 'I', 'couldn', '’', 't', 'hold', 'her', '.'], ['Left', 'her', 'baggage', 'for', 'Rimowa', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 문장들에 대한 단어 토큰화\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장들에 대해 문장별 단어 토큰화 수행.\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50586f3e",
   "metadata": {},
   "source": [
    "#### 영어 스탑워즈 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5ac7fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57963de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수:  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# 영어 stopwords 확인\n",
    "print('영어 stop words 갯수: ', len(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "# 영어 stopwords 중 10개만 출력해보자\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46c150e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ai', \"n't\", 'sure', 'yet.but', \"'m\", 'ya', '.'], ['could', 'want', ',', 'could', 'wish', '.'], ['nights', 'alone', 'miss', '.'], ['days', 'save', 'souvenirs', '.'], [\"'s\", 'time', ',', 'wan', 'na', 'make', 'time', '.'], ['give', 'whole', 'life', '.'], ['left', 'girl', ',', '’', 'mallorca', '.'], ['hate', 'leave', ',', 'call', 'torture', '.'], ['remember', '’', 'hold', '.'], ['left', 'baggage', 'rimowa', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 위 예제 3개 문장에서 얻은 단어 토큰에 대해 stopwords 제거해보자\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        # 소문자로 모두 변환합니다.\n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b4cf50",
   "metadata": {},
   "source": [
    "### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "983ecdeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmmer = LancasterStemmer()\n",
    "\n",
    "print(stemmmer.stem('working'),stemmmer.stem('works'),stemmmer.stem('worked'))\n",
    "print(stemmmer.stem('amusing'),stemmmer.stem('amuses'),stemmmer.stem('amused'))\n",
    "print(stemmmer.stem('happier'),stemmmer.stem('happiest'))\n",
    "print(stemmmer.stem('fancier'),stemmmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55abd838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f74c7d",
   "metadata": {},
   "source": [
    "Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34b37805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Matrix'), ('Matrix', 'is'), ('is', 'everywhere'), ('everywhere', 'its'), ('its', 'all'), ('all', 'around'), ('around', 'us'), ('us', ','), (',', 'here'), ('here', 'even'), ('even', 'in'), ('in', 'this'), ('this', 'room'), ('room', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "all_ngrams = ngrams(words, 2)\n",
    "ngrams = [ngram for ngram in all_ngrams]\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4f37b",
   "metadata": {},
   "source": [
    "## 8.3 Bag of Words – BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe44446f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.                   You can see it out your window or on your television.                   You feel it when you go to work, or go to church or pay your taxes.', 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'] \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "text_sample_01 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                  You can see it out your window or on your television. \\\n",
    "                  You feel it when you go to work, or go to church or pay your taxes.'\n",
    "text_sample_02 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe\\\n",
    "                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "text=[]\n",
    "text.append(text_sample_01); text.append(text_sample_02)\n",
    "## 한줄에 실행 순서대로 하나씩 시키는 표현 (;)세미콜론\n",
    "print(text,\"\\n\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54e4fd",
   "metadata": {},
   "source": [
    "CountVectorizer객체 생성 후 fit(), transform()으로 텍스트에 대한 feature vectorization 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99919fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27630afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_vect = cnt_vect.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f26dd",
   "metadata": {},
   "source": [
    "피처 벡터화 후 데이터 유형 및 여러 속성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97b15b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 51)\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t3\n",
      "  (0, 25)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 40)\t2\n",
      "  :\t:\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t2\n",
      "  (1, 27)\t2\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 35)\t2\n",
      "  (1, 38)\t4\n",
      "  (1, 40)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 47)\t1\n",
      "  (1, 49)\t7\n",
      "  (1, 50)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(ftr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "100f2e5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderland': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n"
     ]
    }
   ],
   "source": [
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "929ab125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 5)\n",
      "{'window': 4, 'pill': 1, 'wake': 2, 'believe': 0, 'want': 3}\n"
     ]
    }
   ],
   "source": [
    "cnt_vect = CountVectorizer(max_features=5, stop_words='english')\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb230f27",
   "metadata": {},
   "source": [
    "ngram_range 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c38c64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 125)\n",
      "{'the': 82, 'matrix': 49, 'is': 42, 'everywhere': 25, 'its': 47, 'all': 0, 'around': 6, 'us': 94, 'here': 32, 'even': 23, 'in': 38, 'this': 88, 'room': 67, 'you': 110, 'can': 15, 'see': 69, 'it': 44, 'out': 57, 'your': 120, 'window': 104, 'or': 53, 'on': 51, 'television': 80, 'feel': 27, 'when': 102, 'go': 29, 'to': 90, 'work': 108, 'church': 17, 'pay': 59, 'taxes': 79, 'the matrix': 84, 'matrix is': 50, 'is everywhere': 43, 'everywhere its': 26, 'its all': 48, 'all around': 1, 'around us': 7, 'us here': 95, 'here even': 33, 'even in': 24, 'in this': 39, 'this room': 89, 'room you': 68, 'you can': 112, 'can see': 16, 'see it': 70, 'it out': 45, 'out your': 58, 'your window': 124, 'window or': 105, 'or on': 55, 'on your': 52, 'your television': 123, 'television you': 81, 'you feel': 113, 'feel it': 28, 'it when': 46, 'when you': 103, 'you go': 114, 'go to': 30, 'to work': 93, 'work or': 109, 'or go': 54, 'to church': 92, 'church or': 18, 'or pay': 56, 'pay your': 60, 'your taxes': 122, 'take': 77, 'blue': 13, 'pill': 61, 'and': 2, 'story': 75, 'ends': 21, 'wake': 96, 'bed': 8, 'believe': 10, 'whatever': 100, 'want': 98, 'red': 65, 'stay': 73, 'wonderland': 106, 'show': 71, 'how': 36, 'deep': 19, 'rabbit': 63, 'hole': 34, 'goes': 31, 'you take': 117, 'take the': 78, 'the blue': 83, 'blue pill': 14, 'pill and': 62, 'and the': 4, 'the story': 87, 'story ends': 76, 'ends you': 22, 'you wake': 118, 'wake in': 97, 'in your': 41, 'your bed': 121, 'bed and': 9, 'and you': 5, 'you believe': 111, 'believe whatever': 11, 'whatever you': 101, 'you want': 119, 'want to': 99, 'to believe': 91, 'believe you': 12, 'the red': 86, 'red pill': 66, 'you stay': 116, 'stay in': 74, 'in wonderland': 40, 'wonderland and': 107, 'and show': 3, 'show you': 72, 'you how': 115, 'how deep': 37, 'deep the': 20, 'the rabbit': 85, 'rabbit hole': 64, 'hole goes': 35}\n"
     ]
    }
   ],
   "source": [
    "cnt_vect = CountVectorizer(ngram_range=(1,2))\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628c485",
   "metadata": {},
   "source": [
    "### COO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a81fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# BOW 압축(COO)을 위한 라이브러리는 NUMPY\n",
    "\n",
    "dense = np.array([[3,0,1],[0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70736e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "## 3은 (0,0)에 위치 / 1은 (0,2)에 위치 / 2는 (1,1)에 위치\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66026a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()\n",
    "# print(sparse_coo)\n",
    "# print(sparse_coo.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f05f2e",
   "metadata": {},
   "source": [
    "### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aee72c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [0,6,0,3,0,0],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]])\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599df81a",
   "metadata": {},
   "source": [
    "#### 간단하게 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b32011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method coo_matrix.toarray of <6x6 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 13 stored elements in COOrdinate format>>\n"
     ]
    }
   ],
   "source": [
    "dense3  = np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [0,6,0,3,0,0],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.coo_matrix(dense3)\n",
    "\n",
    "print(coo.toarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58440b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
